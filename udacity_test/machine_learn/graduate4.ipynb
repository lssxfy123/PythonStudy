{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 自己构建的网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_files       \n",
    "from keras.utils import np_utils\n",
    "from keras.layers import Lambda\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras.applications.resnet50 import preprocess_input as resnet50_pre\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.applications.inception_v3 import preprocess_input as inceptionV3_pre\n",
    "from keras.applications.xception import Xception\n",
    "from keras.applications.xception import preprocess_input as xception_pre\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.applications.vgg16 import preprocess_input as vgg16_pre\n",
    "from keras.applications.vgg19 import VGG19\n",
    "from keras.applications.vgg19 import preprocess_input as vgg19_pre\n",
    "from keras.preprocessing import image   \n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from tqdm import tqdm\n",
    "from PIL import ImageFile  \n",
    "from keras.applications.resnet50 import preprocess_input, decode_predictions\n",
    "from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.layers import Dropout, Flatten, Dense, Input, Activation\n",
    "from keras.models import Sequential, Model\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import ModelCheckpoint  \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "import cv2                \n",
    "import matplotlib.pyplot as plt    \n",
    "import matplotlib.image as mpimg\n",
    "%matplotlib inline \n",
    "import random\n",
    "import os\n",
    "import shutil\n",
    "import h5py\n",
    "import common\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 对图片进行分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "common.divide_images()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "models_and_features = {}\n",
    "models_and_test_features = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batch_extract_features():\n",
    "    \n",
    "    # VGG16\n",
    "    base_model = VGG16(weights='imagenet', include_top=False, pooling='avg')\n",
    "    features_name = common.extract_features(base_model, (224, 224), vgg16_pre)\n",
    "    models_and_features['vgg16'] = features_name\n",
    "    \n",
    "    # VGG19\n",
    "    base_model = VGG19(weights='imagenet', include_top=False, pooling='avg')\n",
    "    features_name = common.extract_features(base_model, (224, 224), vgg19_pre)\n",
    "    models_and_features['vgg19'] = features_name\n",
    "    \n",
    "    # ResNet50\n",
    "    base_model = ResNet50(weights='imagenet', include_top=False, pooling='avg')\n",
    "    features_name = common.extract_features(base_model, (224, 224), resnet50_pre)\n",
    "    models_and_features['resnet50'] = features_name\n",
    "    \n",
    "    # InceptionV3\n",
    "    base_model = InceptionV3(weights='imagenet', include_top=False, pooling='avg')\n",
    "    features_name = common.extract_features(base_model, (299, 299), inceptionV3_pre)\n",
    "    models_and_features['inceptionV3'] = features_name\n",
    "    \n",
    "    # Xception\n",
    "    base_model = Xception(weights='imagenet', include_top=False, pooling='avg')\n",
    "    features_name = common.extract_features(base_model, (299, 299), xception_pre)\n",
    "    models_and_features['xception'] = features_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batch_extract_test_features():\n",
    "    \n",
    "    # VGG16\n",
    "    base_model = VGG16(weights='imagenet', include_top=False, pooling='avg')\n",
    "    features_name = common.extract_test_features(base_model, (224, 224), vgg16_pre)\n",
    "    models_and_test_features['vgg16'] = features_name\n",
    "    \n",
    "    # VGG19\n",
    "    base_model = VGG19(weights='imagenet', include_top=False, pooling='avg')\n",
    "    features_name = common.extract_test_features(base_model, (224, 224), vgg19_pre)\n",
    "    models_and_test_features['vgg19'] = features_name\n",
    "    \n",
    "    # ResNet50\n",
    "    base_model = ResNet50(weights='imagenet', include_top=False, pooling='avg')\n",
    "    features_name = common.extract_test_features(base_model, (224, 224), resnet50_pre)\n",
    "    models_and_test_features['resnet50'] = features_name\n",
    "    \n",
    "    # InceptionV3\n",
    "    base_model = InceptionV3(weights='imagenet', include_top=False, pooling='avg')\n",
    "    features_name = common.extract_test_features(base_model, (299, 299), inceptionV3_pre)\n",
    "    models_and_test_features['inceptionV3'] = features_name\n",
    "    \n",
    "    # Xception\n",
    "    base_model = Xception(weights='imagenet', include_top=False, pooling='avg')\n",
    "    features_name = common.extract_test_features(base_model, (299, 299), xception_pre)\n",
    "    models_and_test_features['xception'] = features_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20000 images belonging to 2 classes.\n",
      "Found 5000 images belonging to 2 classes.\n",
      "1000/1000 [==============================] - 162s 162ms/step\n",
      "250/250 [==============================] - 40s 160ms/step\n",
      "Found 20000 images belonging to 2 classes.\n",
      "Found 5000 images belonging to 2 classes.\n",
      "1000/1000 [==============================] - 191s 191ms/step\n",
      "250/250 [==============================] - 48s 191ms/step\n",
      "Found 20000 images belonging to 2 classes.\n",
      "Found 5000 images belonging to 2 classes.\n",
      "1000/1000 [==============================] - 170s 170ms/step\n",
      "250/250 [==============================] - 42s 169ms/step\n",
      "Found 20000 images belonging to 2 classes.\n",
      "Found 5000 images belonging to 2 classes.\n",
      "1000/1000 [==============================] - 204s 204ms/step\n",
      "250/250 [==============================] - 51s 204ms/step\n",
      "Found 20000 images belonging to 2 classes.\n",
      "Found 5000 images belonging to 2 classes.\n",
      "1000/1000 [==============================] - 338s 338ms/step\n",
      "250/250 [==============================] - 84s 337ms/step\n"
     ]
    }
   ],
   "source": [
    "batch_extract_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 12500 images belonging to 1 classes.\n",
      "625/625 [==============================] - 114s 183ms/step\n",
      "Found 12500 images belonging to 1 classes.\n",
      "625/625 [==============================] - 122s 195ms/step\n",
      "Found 12500 images belonging to 1 classes.\n",
      "625/625 [==============================] - 107s 171ms/step\n",
      "Found 12500 images belonging to 1 classes.\n",
      "625/625 [==============================] - 129s 207ms/step\n",
      "Found 12500 images belonging to 1 classes.\n",
      "625/625 [==============================] - 213s 341ms/step\n"
     ]
    }
   ],
   "source": [
    "batch_extract_test_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if len(models_and_features) == 0:\n",
    "    models_and_features['vgg16'] = 'vgg16_features.npz'\n",
    "    models_and_features['vgg19'] = 'vgg19_features.npz'\n",
    "    models_and_features['resnet50'] = 'resnet50_features.npz'\n",
    "    models_and_features['inceptionV3'] = 'inception_v3_features.npz'\n",
    "    models_and_features['xception'] = 'xception_features.npz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if len(models_and_test_features) == 0:\n",
    "    models_and_test_features['vgg16'] = 'test_vgg16_features.npz'\n",
    "    models_and_test_features['vgg19'] = 'test_vgg19_features.npz'\n",
    "    models_and_test_features['resnet50'] = 'test_resnet50_features.npz'\n",
    "    models_and_test_features['inceptionV3'] = 'test_inception_v3_features.npz'\n",
    "    models_and_test_features['xception'] = 'test_xception_features.npz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Model vgg16\n",
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/10\n",
      "20000/20000 [==============================] - 15s 734us/step - loss: 1.0117 - acc: 0.9325 - val_loss: 0.6427 - val_acc: 0.9596\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.64273, saving model to vgg16.hdf5\n",
      "Epoch 2/10\n",
      "20000/20000 [==============================] - 12s 597us/step - loss: 1.5602 - acc: 0.9023 - val_loss: 0.4897 - val_acc: 0.9692\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.64273 to 0.48968, saving model to vgg16.hdf5\n",
      "Epoch 3/10\n",
      "20000/20000 [==============================] - 12s 597us/step - loss: 2.0916 - acc: 0.8688 - val_loss: 0.5889 - val_acc: 0.9632\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.48968\n",
      "Epoch 4/10\n",
      "20000/20000 [==============================] - 12s 596us/step - loss: 1.5013 - acc: 0.9067 - val_loss: 0.9779 - val_acc: 0.9392\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.48968\n",
      "Epoch 5/10\n",
      "20000/20000 [==============================] - 12s 596us/step - loss: 2.0072 - acc: 0.8753 - val_loss: 2.0179 - val_acc: 0.8748\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.48968\n",
      "Epoch 6/10\n",
      "20000/20000 [==============================] - 12s 597us/step - loss: 1.8647 - acc: 0.8840 - val_loss: 2.3145 - val_acc: 0.8548\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.48968\n",
      "Epoch 7/10\n",
      "20000/20000 [==============================] - 12s 596us/step - loss: 1.4814 - acc: 0.9071 - val_loss: 4.3262 - val_acc: 0.7284\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.48968\n",
      "Epoch 8/10\n",
      "20000/20000 [==============================] - 12s 596us/step - loss: 2.5727 - acc: 0.8386 - val_loss: 0.9442 - val_acc: 0.9414\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.48968\n",
      "Epoch 9/10\n",
      "20000/20000 [==============================] - 12s 596us/step - loss: 1.5827 - acc: 0.9011 - val_loss: 1.8635 - val_acc: 0.8830\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.48968\n",
      "Epoch 10/10\n",
      "20000/20000 [==============================] - 12s 596us/step - loss: 1.8071 - acc: 0.8868 - val_loss: 0.5775 - val_acc: 0.9638\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.48968\n",
      "Train Model vgg19\n",
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/10\n",
      "20000/20000 [==============================] - 15s 734us/step - loss: 1.1927 - acc: 0.9217 - val_loss: 7.7733 - val_acc: 0.5176\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 7.77325, saving model to vgg19.hdf5\n",
      "Epoch 2/10\n",
      "20000/20000 [==============================] - 12s 597us/step - loss: 1.4343 - acc: 0.9103 - val_loss: 0.3567 - val_acc: 0.9776\n",
      "\n",
      "Epoch 00002: val_loss improved from 7.77325 to 0.35672, saving model to vgg19.hdf5\n",
      "Epoch 3/10\n",
      "20000/20000 [==============================] - 12s 597us/step - loss: 1.1225 - acc: 0.9298 - val_loss: 3.7316 - val_acc: 0.7682\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.35672\n",
      "Epoch 4/10\n",
      "20000/20000 [==============================] - 12s 597us/step - loss: 1.2183 - acc: 0.9242 - val_loss: 0.6604 - val_acc: 0.9590\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.35672\n",
      "Epoch 5/10\n",
      "20000/20000 [==============================] - 12s 596us/step - loss: 1.0401 - acc: 0.9351 - val_loss: 0.9764 - val_acc: 0.9388\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.35672\n",
      "Epoch 6/10\n",
      "20000/20000 [==============================] - 12s 596us/step - loss: 0.9057 - acc: 0.9435 - val_loss: 0.9698 - val_acc: 0.9392\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.35672\n",
      "Epoch 7/10\n",
      "20000/20000 [==============================] - 12s 598us/step - loss: 1.2162 - acc: 0.9241 - val_loss: 0.5116 - val_acc: 0.9682\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.35672\n",
      "Epoch 8/10\n",
      "20000/20000 [==============================] - 12s 597us/step - loss: 0.5279 - acc: 0.9672 - val_loss: 0.5116 - val_acc: 0.9682\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.35672\n",
      "Epoch 9/10\n",
      "20000/20000 [==============================] - 12s 596us/step - loss: 1.0214 - acc: 0.9365 - val_loss: 4.3580 - val_acc: 0.7296\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.35672\n",
      "Epoch 10/10\n",
      "20000/20000 [==============================] - 12s 597us/step - loss: 2.9140 - acc: 0.8182 - val_loss: 0.6545 - val_acc: 0.9590\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.35672\n",
      "Train Model resnet50\n",
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/10\n",
      "20000/20000 [==============================] - 18s 904us/step - loss: 0.6456 - acc: 0.9540 - val_loss: 0.2611 - val_acc: 0.9832\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.26106, saving model to resnet50.hdf5\n",
      "Epoch 2/10\n",
      "20000/20000 [==============================] - 15s 765us/step - loss: 0.9905 - acc: 0.9377 - val_loss: 0.7156 - val_acc: 0.9556\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.26106\n",
      "Epoch 3/10\n",
      "20000/20000 [==============================] - 15s 763us/step - loss: 0.6985 - acc: 0.9561 - val_loss: 0.5870 - val_acc: 0.9630\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.26106\n",
      "Epoch 4/10\n",
      "20000/20000 [==============================] - 15s 763us/step - loss: 0.6817 - acc: 0.9572 - val_loss: 0.5328 - val_acc: 0.9666\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.26106\n",
      "Epoch 5/10\n",
      "20000/20000 [==============================] - 15s 763us/step - loss: 0.6717 - acc: 0.9578 - val_loss: 0.3706 - val_acc: 0.9766\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.26106\n",
      "Epoch 6/10\n",
      "20000/20000 [==============================] - 15s 764us/step - loss: 0.4187 - acc: 0.9737 - val_loss: 0.3673 - val_acc: 0.9770\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.26106\n",
      "Epoch 7/10\n",
      "20000/20000 [==============================] - 15s 765us/step - loss: 1.0572 - acc: 0.9342 - val_loss: 0.3143 - val_acc: 0.9804\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.26106\n",
      "Epoch 8/10\n",
      "20000/20000 [==============================] - 15s 764us/step - loss: 0.6033 - acc: 0.9622 - val_loss: 0.2862 - val_acc: 0.9820\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.26106\n",
      "Epoch 9/10\n",
      "20000/20000 [==============================] - 15s 765us/step - loss: 0.4004 - acc: 0.9750 - val_loss: 0.3312 - val_acc: 0.9794\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.26106\n",
      "Epoch 10/10\n",
      "20000/20000 [==============================] - 15s 764us/step - loss: 0.5915 - acc: 0.9629 - val_loss: 0.4991 - val_acc: 0.9690\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.26106\n",
      "Train Model inceptionV3\n",
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/10\n",
      "20000/20000 [==============================] - 18s 909us/step - loss: 0.2339 - acc: 0.9769 - val_loss: 0.1474 - val_acc: 0.9902\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.14739, saving model to inceptionV3.hdf5\n",
      "Epoch 2/10\n",
      "20000/20000 [==============================] - 15s 765us/step - loss: 0.2308 - acc: 0.9846 - val_loss: 0.1760 - val_acc: 0.9886\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.14739\n",
      "Epoch 3/10\n",
      "20000/20000 [==============================] - 15s 764us/step - loss: 0.3449 - acc: 0.9779 - val_loss: 0.1337 - val_acc: 0.9912\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.14739 to 0.13368, saving model to inceptionV3.hdf5\n",
      "Epoch 4/10\n",
      "20000/20000 [==============================] - 15s 764us/step - loss: 0.2862 - acc: 0.9818 - val_loss: 0.1909 - val_acc: 0.9880\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.13368\n",
      "Epoch 5/10\n",
      "20000/20000 [==============================] - 15s 766us/step - loss: 0.3797 - acc: 0.9761 - val_loss: 0.1501 - val_acc: 0.9906\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.13368\n",
      "Epoch 6/10\n",
      "20000/20000 [==============================] - 15s 764us/step - loss: 0.2939 - acc: 0.9814 - val_loss: 0.1298 - val_acc: 0.9918\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.13368 to 0.12980, saving model to inceptionV3.hdf5\n",
      "Epoch 7/10\n",
      "20000/20000 [==============================] - 15s 767us/step - loss: 0.3118 - acc: 0.9803 - val_loss: 0.4383 - val_acc: 0.9724\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.12980\n",
      "Epoch 8/10\n",
      "20000/20000 [==============================] - 15s 765us/step - loss: 0.5732 - acc: 0.9639 - val_loss: 0.3763 - val_acc: 0.9764\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.12980\n",
      "Epoch 9/10\n",
      "20000/20000 [==============================] - 15s 765us/step - loss: 0.4038 - acc: 0.9746 - val_loss: 0.2838 - val_acc: 0.9822\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.12980\n",
      "Epoch 10/10\n",
      "20000/20000 [==============================] - 15s 764us/step - loss: 0.5577 - acc: 0.9651 - val_loss: 0.2345 - val_acc: 0.9852\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.12980\n",
      "Train Model xception\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/10\n",
      "20000/20000 [==============================] - 18s 911us/step - loss: 0.2181 - acc: 0.9825 - val_loss: 0.3781 - val_acc: 0.9760\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.37807, saving model to xception.hdf5\n",
      "Epoch 2/10\n",
      "20000/20000 [==============================] - 15s 767us/step - loss: 0.2281 - acc: 0.9852 - val_loss: 0.2346 - val_acc: 0.9852\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.37807 to 0.23458, saving model to xception.hdf5\n",
      "Epoch 3/10\n",
      "20000/20000 [==============================] - 15s 768us/step - loss: 0.3111 - acc: 0.9801 - val_loss: 0.7359 - val_acc: 0.9532\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.23458\n",
      "Epoch 4/10\n",
      "20000/20000 [==============================] - 15s 766us/step - loss: 0.2866 - acc: 0.9819 - val_loss: 0.1671 - val_acc: 0.9894\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.23458 to 0.16713, saving model to xception.hdf5\n",
      "Epoch 5/10\n",
      "20000/20000 [==============================] - 15s 767us/step - loss: 0.2472 - acc: 0.9844 - val_loss: 0.1056 - val_acc: 0.9934\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.16713 to 0.10564, saving model to xception.hdf5\n",
      "Epoch 6/10\n",
      "20000/20000 [==============================] - 15s 767us/step - loss: 0.1527 - acc: 0.9904 - val_loss: 0.1023 - val_acc: 0.9936\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.10564 to 0.10231, saving model to xception.hdf5\n",
      "Epoch 7/10\n",
      "20000/20000 [==============================] - 15s 767us/step - loss: 0.3317 - acc: 0.9792 - val_loss: 0.3005 - val_acc: 0.9810\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.10231\n",
      "Epoch 8/10\n",
      "20000/20000 [==============================] - 15s 766us/step - loss: 0.2482 - acc: 0.9844 - val_loss: 0.0992 - val_acc: 0.9938\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.10231 to 0.09919, saving model to xception.hdf5\n",
      "Epoch 9/10\n",
      "20000/20000 [==============================] - 15s 767us/step - loss: 0.1365 - acc: 0.9914 - val_loss: 0.0992 - val_acc: 0.9938\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.09919\n",
      "Epoch 10/10\n",
      "20000/20000 [==============================] - 15s 767us/step - loss: 0.1431 - acc: 0.9910 - val_loss: 0.1058 - val_acc: 0.9934\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.09919\n"
     ]
    }
   ],
   "source": [
    "# 训练+预测\n",
    "for key, value in models_and_features.items():\n",
    "    print('Train Model {0}'.format(key))\n",
    "    features = np.load(value)\n",
    "    features_test = np.load(models_and_test_features[key])\n",
    "    train_features = features['train']\n",
    "    train_labels = features['train_label']\n",
    "    valid_features = features['valid']\n",
    "    valid_labels = features['valid_label']\n",
    "    test_features = features_test['test']\n",
    "    test_filenames = features_test['test_filename']\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Dense(2048, input_shape=(train_features.shape[1],), activation='relu'))\n",
    "    model.add(Dropout(.5))\n",
    "    model.add(Dense(2048, activation='relu'))\n",
    "    model.add(Dropout(.5))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(Adam(lr=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    checkpointer = ModelCheckpoint(filepath='{0}.hdf5'.format(key), verbose=1, save_best_only=True)\n",
    "    train_result = model.fit(train_features, train_labels, epochs=10, batch_size=common.batch_size,\n",
    "          validation_data=(valid_features, valid_labels), verbose=1, callbacks=[checkpointer])\n",
    "    \n",
    "    with open('{0}_history.pkl'.format(key), 'wb') as f:\n",
    "        pickle.dump(train_result.history, f)\n",
    "    model.load_weights('{0}.hdf5'.format(key))\n",
    "    prediction = model.predict(test_features, batch_size=common.batch_size)\n",
    "    prediction = prediction[:, 0].clip(0.01, 0.99)\n",
    "    test_fileindex = np.array([os.path.splitext(os.path.split(filename)[1])[0] for filename in test_filenames])\n",
    "    data = np.stack([test_fileindex, prediction], axis=1)\n",
    "    tmp = pd.DataFrame(data, columns=['id', 'label'])\n",
    "    tmp['id'] = tmp['id'].apply(pd.to_numeric)\n",
    "    submit_frame = pd.read_csv('dogs-vs-cats/sample_submission.csv')\n",
    "    result = pd.merge(submit_frame, tmp, on=\"id\", how='left')\n",
    "    result = result.rename(index=str, columns={\"label_y\": \"label\"})\n",
    "    result[['id','label']].to_csv('{0}_predict.csv'.format(key),index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
