{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 自己构建的网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_files       \n",
    "from keras.utils import np_utils\n",
    "from keras.layers import Lambda\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras.applications.resnet50 import preprocess_input as resnet50_pre\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.applications.inception_v3 import preprocess_input as inceptionV3_pre\n",
    "from keras.applications.xception import Xception\n",
    "from keras.applications.xception import preprocess_input as xception_pre\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.applications.vgg16 import preprocess_input as vgg16_pre\n",
    "from keras.applications.vgg19 import VGG19\n",
    "from keras.applications.vgg19 import preprocess_input as vgg19_pre\n",
    "from keras.preprocessing import image   \n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from tqdm import tqdm\n",
    "from PIL import ImageFile  \n",
    "from keras.applications.resnet50 import preprocess_input, decode_predictions\n",
    "from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.layers import Dropout, Flatten, Dense, Input, Activation\n",
    "from keras.models import Sequential, Model\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import ModelCheckpoint  \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "import cv2                \n",
    "import matplotlib.pyplot as plt    \n",
    "import matplotlib.image as mpimg\n",
    "%matplotlib inline \n",
    "import random\n",
    "import os\n",
    "import shutil\n",
    "import h5py\n",
    "import common\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 对图片进行分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "common.divide_images()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_and_features = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_extract_features():\n",
    "    \n",
    "    # VGG16\n",
    "    base_model = VGG16(weights='imagenet', include_top=False, pooling='avg')\n",
    "    features_name = common.extract_features(base_model, (224, 224), vgg16_pre)\n",
    "    models_and_features['vgg16'] = features_name\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 160 images belonging to 2 classes.\n",
      "Found 40 images belonging to 2 classes.\n",
      "Found 100 images belonging to 1 classes.\n",
      "8/8 [==============================] - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 47s - ETA: 31 - ETA: 15 - 128s 16s/step\n",
      "2/2 [==============================] - ETA: 23 - 47s 24s/step\n",
      "5/5 [==============================] - ETA: 1: - ETA: 1: - ETA: 58s - ETA: 29 - 152s 30s/step\n"
     ]
    }
   ],
   "source": [
    "batch_extract_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(models_and_features) == 0:\n",
    "    models_and_features['vgg16'] = ''\n",
    "    models_and_features['vgg19'] = ''\n",
    "    models_and_features['resnet50'] = ''\n",
    "    models_and_features['inceptionV3'] = ''\n",
    "    models_and_features['xception'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 160 samples, validate on 40 samples\n",
      "Epoch 1/2\n",
      "160/160 [==============================] - ETA: 8s - loss: 2.0720 - acc: 0.550 - ETA: 4s - loss: 3.9783 - acc: 0.500 - ETA: 2s - loss: 3.1109 - acc: 0.600 - ETA: 1s - loss: 2.5358 - acc: 0.662 - ETA: 1s - loss: 2.0291 - acc: 0.730 - ETA: 0s - loss: 1.7307 - acc: 0.758 - ETA: 0s - loss: 1.4834 - acc: 0.792 - 2s 14ms/step - loss: 1.3987 - acc: 0.8125 - val_loss: 0.0127 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.01265, saving model to vgg16.hdf5\n",
      "Epoch 2/2\n",
      "160/160 [==============================] - ETA: 0s - loss: 0.5691 - acc: 0.950 - ETA: 0s - loss: 0.2846 - acc: 0.975 - ETA: 0s - loss: 0.2591 - acc: 0.950 - ETA: 0s - loss: 0.2714 - acc: 0.937 - ETA: 0s - loss: 0.3575 - acc: 0.930 - ETA: 0s - loss: 0.2982 - acc: 0.941 - ETA: 0s - loss: 0.2851 - acc: 0.942 - 1s 6ms/step - loss: 0.3304 - acc: 0.9437 - val_loss: 0.0019 - val_acc: 1.0000\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.01265 to 0.00188, saving model to vgg16.hdf5\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for key, value in models_and_features.items():\n",
    "    features = np.load(value)\n",
    "    train_features = features['train']\n",
    "    train_labels = features['train_label']\n",
    "    valid_features = features['valid']\n",
    "    valid_labels = features['valid_label']\n",
    "    test_features = features['test']\n",
    "    test_filenames = features['test_filename']\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Dense(2048, input_shape=(train_features.shape[1],), activation='relu'))\n",
    "    model.add(Dropout(.5))\n",
    "    model.add(Dense(2048, activation='relu'))\n",
    "    model.add(Dropout(.5))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(Adam(lr=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    checkpointer = ModelCheckpoint(filepath='{0}.hdf5'.format(key), verbose=1, save_best_only=True)\n",
    "    train_result = model.fit(train_features, train_labels, epochs=2, batch_size=common.batch_size,\n",
    "          validation_data=(valid_features, valid_labels), verbose=1, callbacks=[checkpointer])\n",
    "    with open('{0}_history.pkl'.format(key), 'wb') as f:\n",
    "        pickle.dump(train_result.history, f)\n",
    "        \n",
    "    model.load_weights('{0}.hdf5'.format(key))\n",
    "    prediction = model.predict(test_features, batch_size=common.batch_size)\n",
    "    prediction = prediction[:, 0].clip(0.01, 0.99)\n",
    "    test_fileindex = np.array([os.path.splitext(os.path.split(filename)[1])[0] for filename in test_filenames])\n",
    "    data = np.stack([test_fileindex, prediction], axis=1)\n",
    "    tmp = pd.DataFrame(data, columns=['id', 'label'])\n",
    "    tmp['id'] = tmp['id'].apply(pd.to_numeric)\n",
    "    submit_frame = pd.read_csv('dogs-vs-cats/sample_submission.csv')\n",
    "    result = pd.merge(submit_frame, tmp, on=\"id\", how='left')\n",
    "    result = result.rename(index=str, columns={\"label_y\": \"label\"})\n",
    "    result[['id','label']].to_csv('{0}_predict.csv'.format(key),index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'val_loss': [0.01265449682250619, 0.0018804915889631957],\n",
       " 'val_acc': [1.0, 1.0],\n",
       " 'loss': [1.3987414118490644, 0.33037288575865364],\n",
       " 'acc': [0.8125, 0.9437499865889549]}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit_result.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('history.pkl', 'wb')\n",
    "pickle.dump(fit_result.history, file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'val_loss': [0.01265449682250619, 0.0018804915889631957],\n",
       " 'val_acc': [1.0, 1.0],\n",
       " 'loss': [1.3987414118490644, 0.33037288575865364],\n",
       " 'acc': [0.8125, 0.9437499865889549]}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('history.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
