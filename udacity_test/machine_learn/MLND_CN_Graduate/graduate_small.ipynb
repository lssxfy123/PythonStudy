{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 自己构建的网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_files       \n",
    "from keras.utils import np_utils\n",
    "from keras.layers import Lambda\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras.applications.resnet50 import preprocess_input as resnet50_pre\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.applications.inception_v3 import preprocess_input as inceptionV3_pre\n",
    "from keras.applications.xception import Xception\n",
    "from keras.applications.xception import preprocess_input as xception_pre\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.applications.vgg16 import preprocess_input as vgg16_pre\n",
    "from keras.applications.vgg19 import VGG19\n",
    "from keras.applications.vgg19 import preprocess_input as vgg19_pre\n",
    "from keras.preprocessing import image   \n",
    "from keras.optimizers import SGD, Adam, RMSprop\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from tqdm import tqdm\n",
    "from PIL import ImageFile  \n",
    "from keras.applications.resnet50 import preprocess_input, decode_predictions\n",
    "from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.layers import Dropout, Flatten, Dense, Input, Activation\n",
    "from keras.models import Sequential, Model\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import ModelCheckpoint  \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "import cv2                \n",
    "import matplotlib.pyplot as plt    \n",
    "import matplotlib.image as mpimg\n",
    "%matplotlib inline \n",
    "import random\n",
    "import os\n",
    "import shutil\n",
    "import h5py\n",
    "import common\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 对图片进行分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "common.divide_images()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_models_and_features = {}\n",
    "small_models_and_test_features = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_extract_small_features():\n",
    "    \n",
    "    # VGG16\n",
    "    base_model = VGG16(weights='imagenet', include_top=False, pooling='avg')\n",
    "    features_name = common.extract_small_features(base_model, (224, 224), vgg16_pre)\n",
    "    small_models_and_features['vgg16'] = features_name\n",
    "    \n",
    "    # VGG19\n",
    "    base_model = VGG19(weights='imagenet', include_top=False, pooling='avg')\n",
    "    features_name = common.extract_small_features(base_model, (224, 224), vgg19_pre)\n",
    "    small_models_and_features['vgg19'] = features_name\n",
    "    \n",
    "    # ResNet50\n",
    "    base_model = ResNet50(weights='imagenet', include_top=False, pooling='avg')\n",
    "    features_name = common.extract_small_features(base_model, (224, 224), resnet50_pre)\n",
    "    small_models_and_features['resnet50'] = features_name\n",
    "    \n",
    "    # InceptionV3\n",
    "    base_model = InceptionV3(weights='imagenet', include_top=False, pooling='avg')\n",
    "    features_name = common.extract_small_features(base_model, (299, 299), inceptionV3_pre)\n",
    "    small_models_and_features['inceptionV3'] = features_name\n",
    "    \n",
    "    # Xception\n",
    "    base_model = Xception(weights='imagenet', include_top=False, pooling='avg')\n",
    "    features_name = common.extract_small_features(base_model, (299, 299), xception_pre)\n",
    "    small_models_and_features['xception'] = features_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_extract_small_test_features():\n",
    "    \n",
    "    # VGG16\n",
    "    base_model = VGG16(weights='imagenet', include_top=False, pooling='avg')\n",
    "    features_name = common.extract_small_test_features(base_model, (224, 224), vgg16_pre)\n",
    "    small_models_and_test_features['vgg16'] = features_name\n",
    "    \n",
    "    # VGG19\n",
    "    base_model = VGG19(weights='imagenet', include_top=False, pooling='avg')\n",
    "    features_name = common.extract_small_test_features(base_model, (224, 224), vgg19_pre)\n",
    "    small_models_and_test_features['vgg19'] = features_name\n",
    "    \n",
    "    # ResNet50\n",
    "    base_model = ResNet50(weights='imagenet', include_top=False, pooling='avg')\n",
    "    features_name = common.extract_small_test_features(base_model, (224, 224), resnet50_pre)\n",
    "    small_models_and_test_features['resnet50'] = features_name\n",
    "    \n",
    "    # InceptionV3\n",
    "    base_model = InceptionV3(weights='imagenet', include_top=False, pooling='avg')\n",
    "    features_name = common.extract_small_test_features(base_model, (299, 299), inceptionV3_pre)\n",
    "    small_models_and_test_features['inceptionV3'] = features_name\n",
    "    \n",
    "    # Xception\n",
    "    base_model = Xception(weights='imagenet', include_top=False, pooling='avg')\n",
    "    features_name = common.extract_small_test_features(base_model, (299, 299), xception_pre)\n",
    "    small_models_and_test_features['xception'] = features_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1600 images belonging to 2 classes.\n",
      "Found 400 images belonging to 2 classes.\n",
      "80/80 [==============================] - ETA: 1:01:3 - ETA: 55:03  - ETA: 53:3 - ETA: 52:2 - ETA: 50:4 - ETA: 49:0 - ETA: 48:2 - ETA: 47:2 - ETA: 46:3 - ETA: 45:2 - ETA: 44:3 - ETA: 43:5 - ETA: 43:0 - ETA: 42:0 - ETA: 41:1 - ETA: 40:4 - ETA: 39:5 - ETA: 39:1 - ETA: 38:4 - ETA: 38:1 - ETA: 37:4 - ETA: 37:0 - ETA: 36:3 - ETA: 35:5 - ETA: 35:1 - ETA: 34:3 - ETA: 33:5 - ETA: 33:1 - ETA: 32:2 - ETA: 31:4 - ETA: 31:0 - ETA: 30:1 - ETA: 29:3 - ETA: 28:5 - ETA: 28:0 - ETA: 27:2 - ETA: 26:4 - ETA: 25:5 - ETA: 25:1 - ETA: 24:2 - ETA: 23:4 - ETA: 23:0 - ETA: 22:2 - ETA: 21:4 - ETA: 20:5 - ETA: 20:2 - ETA: 19:4 - ETA: 19:0 - ETA: 18:2 - ETA: 17:4 - ETA: 17:0 - ETA: 16:2 - ETA: 15:4 - ETA: 15:1 - ETA: 14:3 - ETA: 13:5 - ETA: 13:2 - ETA: 12:4 - ETA: 12:0 - ETA: 11:3 - ETA: 10:5 - ETA: 10:2 - ETA: 9:4 - ETA: 9: - ETA: 8: - ETA: 7: - ETA: 7: - ETA: 6: - ETA: 6: - ETA: 5: - ETA: 5: - ETA: 4: - ETA: 3: - ETA: 3: - ETA: 2: - ETA: 2: - ETA: 1: - ETA: 1: - ETA: 33s - 2690s 34s/step\n",
      "20/20 [==============================] - ETA: 10:4 - ETA: 9:4 - ETA: 9: - ETA: 8: - ETA: 7: - ETA: 7: - ETA: 6: - ETA: 6: - ETA: 5: - ETA: 5: - ETA: 4: - ETA: 4: - ETA: 3: - ETA: 3: - ETA: 2: - ETA: 2: - ETA: 1: - ETA: 1: - ETA: 30s - 606s 30s/step\n",
      "Found 1600 images belonging to 2 classes.\n",
      "Found 400 images belonging to 2 classes.\n",
      "80/80 [==============================] - ETA: 53:4 - ETA: 52:2 - ETA: 50:5 - ETA: 49:4 - ETA: 48:5 - ETA: 47:5 - ETA: 46:5 - ETA: 46:3 - ETA: 45:4 - ETA: 45:0 - ETA: 44:2 - ETA: 43:5 - ETA: 43:2 - ETA: 42:3 - ETA: 41:5 - ETA: 41:1 - ETA: 40:3 - ETA: 39:5 - ETA: 39:1 - ETA: 38:3 - ETA: 37:5 - ETA: 37:1 - ETA: 36:4 - ETA: 36:0 - ETA: 35:2 - ETA: 34:4 - ETA: 34:0 - ETA: 33:2 - ETA: 32:4 - ETA: 32:0 - ETA: 31:3 - ETA: 30:5 - ETA: 30:1 - ETA: 29:4 - ETA: 29:0 - ETA: 28:2 - ETA: 27:4 - ETA: 27:0 - ETA: 26:2 - ETA: 25:4 - ETA: 25:0 - ETA: 24:3 - ETA: 23:5 - ETA: 23:1 - ETA: 22:3 - ETA: 21:5 - ETA: 21:1 - ETA: 20:3 - ETA: 20:0 - ETA: 19:2 - ETA: 18:4 - ETA: 18:0 - ETA: 17:2 - ETA: 16:4 - ETA: 16:0 - ETA: 15:3 - ETA: 14:5 - ETA: 14:1 - ETA: 13:3 - ETA: 12:5 - ETA: 12:1 - ETA: 11:3 - ETA: 10:5 - ETA: 10:2 - ETA: 9:4 - ETA: 9: - ETA: 8: - ETA: 7: - ETA: 7: - ETA: 6: - ETA: 5: - ETA: 5: - ETA: 4: - ETA: 3: - ETA: 3: - ETA: 2: - ETA: 1: - ETA: 1: - ETA: 38s - 3109s 39s/step\n",
      "20/20 [==============================] - ETA: 12:0 - ETA: 11:3 - ETA: 10:4 - ETA: 10:1 - ETA: 9:4 - ETA: 9: - ETA: 8: - ETA: 7: - ETA: 7: - ETA: 6: - ETA: 5: - ETA: 5: - ETA: 4: - ETA: 3: - ETA: 3: - ETA: 2: - ETA: 1: - ETA: 1: - ETA: 38s - 780s 39s/step\n",
      "Found 1600 images belonging to 2 classes.\n",
      "Found 400 images belonging to 2 classes.\n",
      "80/80 [==============================] - ETA: 30:0 - ETA: 28:2 - ETA: 27:2 - ETA: 27:0 - ETA: 26:2 - ETA: 26:0 - ETA: 26:1 - ETA: 25:4 - ETA: 25:1 - ETA: 24:4 - ETA: 24:1 - ETA: 23:5 - ETA: 23:3 - ETA: 23:1 - ETA: 23:0 - ETA: 22:2 - ETA: 22:1 - ETA: 21:4 - ETA: 21:3 - ETA: 21:1 - ETA: 20:5 - ETA: 20:3 - ETA: 20:1 - ETA: 19:4 - ETA: 19:2 - ETA: 19:0 - ETA: 18:4 - ETA: 18:2 - ETA: 18:0 - ETA: 17:3 - ETA: 17:1 - ETA: 16:5 - ETA: 16:3 - ETA: 16:1 - ETA: 15:5 - ETA: 15:3 - ETA: 15:1 - ETA: 14:5 - ETA: 14:3 - ETA: 14:0 - ETA: 13:4 - ETA: 13:2 - ETA: 13:0 - ETA: 12:4 - ETA: 12:2 - ETA: 12:0 - ETA: 11:4 - ETA: 11:2 - ETA: 10:5 - ETA: 10:3 - ETA: 10:1 - ETA: 9:5 - ETA: 9: - ETA: 9: - ETA: 8: - ETA: 8: - ETA: 8: - ETA: 7: - ETA: 7: - ETA: 7: - ETA: 6: - ETA: 6: - ETA: 6: - ETA: 5: - ETA: 5: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 42s - ETA: 21 - 1713s 21s/step\n",
      "20/20 [==============================] - ETA: 7: - ETA: 6: - ETA: 6: - ETA: 5: - ETA: 5: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 42s - ETA: 21 - 428s 21s/step\n",
      "Found 1600 images belonging to 2 classes.\n",
      "Found 400 images belonging to 2 classes.\n",
      "80/80 [==============================] - ETA: 39:5 - ETA: 35:3 - ETA: 31:5 - ETA: 30:4 - ETA: 29:5 - ETA: 28:5 - ETA: 28:1 - ETA: 27:4 - ETA: 27:1 - ETA: 26:4 - ETA: 26:2 - ETA: 25:4 - ETA: 25:1 - ETA: 24:5 - ETA: 24:2 - ETA: 24:1 - ETA: 23:4 - ETA: 23:2 - ETA: 22:5 - ETA: 22:2 - ETA: 22:0 - ETA: 21:3 - ETA: 21:1 - ETA: 20:5 - ETA: 20:2 - ETA: 20:0 - ETA: 19:4 - ETA: 19:2 - ETA: 18:5 - ETA: 18:2 - ETA: 18:0 - ETA: 17:4 - ETA: 17:1 - ETA: 16:5 - ETA: 16:3 - ETA: 16:1 - ETA: 15:5 - ETA: 15:2 - ETA: 15:0 - ETA: 14:4 - ETA: 14:2 - ETA: 13:5 - ETA: 13:3 - ETA: 13:1 - ETA: 12:5 - ETA: 12:2 - ETA: 12:0 - ETA: 11:4 - ETA: 11:2 - ETA: 11:0 - ETA: 10:3 - ETA: 10:1 - ETA: 9:5 - ETA: 9: - ETA: 9: - ETA: 8: - ETA: 8: - ETA: 8: - ETA: 7: - ETA: 7: - ETA: 6: - ETA: 6: - ETA: 6: - ETA: 5: - ETA: 5: - ETA: 5: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 3: - ETA: 3: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 43s - ETA: 21 - 1756s 22s/step\n",
      "20/20 [==============================] - ETA: 6: - ETA: 6: - ETA: 6: - ETA: 5: - ETA: 5: - ETA: 5: - ETA: 4: - ETA: 4: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 44s - ETA: 22 - 445s 22s/step\n",
      "Found 1600 images belonging to 2 classes.\n",
      "Found 400 images belonging to 2 classes.\n",
      "80/80 [==============================] - ETA: 1:04:1 - ETA: 1:01:0 - ETA: 57:19  - ETA: 58:2 - ETA: 56:4 - ETA: 55:4 - ETA: 54:4 - ETA: 53:5 - ETA: 53:0 - ETA: 52:0 - ETA: 51:2 - ETA: 50:2 - ETA: 49:4 - ETA: 49:0 - ETA: 48:2 - ETA: 47:2 - ETA: 46:3 - ETA: 45:5 - ETA: 45:0 - ETA: 44:3 - ETA: 43:5 - ETA: 43:0 - ETA: 42:1 - ETA: 41:3 - ETA: 40:5 - ETA: 40:0 - ETA: 39:1 - ETA: 38:3 - ETA: 37:5 - ETA: 37:0 - ETA: 36:1 - ETA: 35:3 - ETA: 34:5 - ETA: 34:0 - ETA: 33:1 - ETA: 32:3 - ETA: 31:5 - ETA: 31:0 - ETA: 30:2 - ETA: 29:3 - ETA: 28:4 - ETA: 28:0 - ETA: 27:2 - ETA: 26:3 - ETA: 25:5 - ETA: 25:0 - ETA: 24:2 - ETA: 23:3 - ETA: 22:5 - ETA: 22:1 - ETA: 21:2 - ETA: 20:4 - ETA: 19:5 - ETA: 19:1 - ETA: 18:2 - ETA: 17:4 - ETA: 16:5 - ETA: 16:1 - ETA: 15:3 - ETA: 14:4 - ETA: 14:0 - ETA: 13:1 - ETA: 12:3 - ETA: 11:5 - ETA: 11:0 - ETA: 10:2 - ETA: 9:3 - ETA: 8: - ETA: 8: - ETA: 7: - ETA: 6: - ETA: 5: - ETA: 5: - ETA: 4: - ETA: 3: - ETA: 2: - ETA: 2: - ETA: 1: - ETA: 44s - 3548s 44s/step\n",
      "20/20 [==============================] - ETA: 14:3 - ETA: 13:2 - ETA: 12:2 - ETA: 11:5 - ETA: 11:1 - ETA: 10:2 - ETA: 9:3 - ETA: 8: - ETA: 8: - ETA: 7: - ETA: 6: - ETA: 5: - ETA: 5: - ETA: 4: - ETA: 3: - ETA: 2: - ETA: 2: - ETA: 1: - ETA: 44s - 882s 44s/step\n"
     ]
    }
   ],
   "source": [
    "batch_extract_small_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 500 images belonging to 1 classes.\n",
      "25/25 [==============================] - ETA: 8: - ETA: 9: - ETA: 9: - ETA: 11:1 - ETA: 12:1 - ETA: 12:4 - ETA: 12:5 - ETA: 12:1 - ETA: 12:0 - ETA: 11:3 - ETA: 10:5 - ETA: 10:1 - ETA: 9:2 - ETA: 8: - ETA: 7: - ETA: 7: - ETA: 6: - ETA: 5: - ETA: 4: - ETA: 3: - ETA: 3: - ETA: 2: - ETA: 1: - ETA: 44s - 1103s 44s/step\n",
      "Found 500 images belonging to 1 classes.\n",
      "25/25 [==============================] - ETA: 20:1 - ETA: 18:3 - ETA: 17:4 - ETA: 16:5 - ETA: 16:0 - ETA: 15:0 - ETA: 14:1 - ETA: 13:2 - ETA: 12:3 - ETA: 11:5 - ETA: 11:0 - ETA: 10:1 - ETA: 9:2 - ETA: 8: - ETA: 7: - ETA: 7: - ETA: 6: - ETA: 5: - ETA: 4: - ETA: 3: - ETA: 3: - ETA: 2: - ETA: 1: - ETA: 46s - 1177s 47s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0410 18:46:55.649117  8116 deprecation.py:506] From C:\\ProgramData\\Anaconda3\\envs\\graduate\\lib\\site-packages\\tensorflow\\python\\training\\moving_averages.py:210: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 500 images belonging to 1 classes.\n",
      "25/25 [==============================] - ETA: 10:5 - ETA: 10:4 - ETA: 9:5 - ETA: 9: - ETA: 8: - ETA: 8: - ETA: 7: - ETA: 7: - ETA: 6: - ETA: 6: - ETA: 6: - ETA: 5: - ETA: 5: - ETA: 4: - ETA: 4: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 2: - ETA: 2: - ETA: 1: - ETA: 1: - ETA: 51s - ETA: 25 - 647s 26s/step\n",
      "Found 500 images belonging to 1 classes.\n",
      "25/25 [==============================] - ETA: 15:2 - ETA: 12:3 - ETA: 11:1 - ETA: 10:0 - ETA: 9:3 - ETA: 8: - ETA: 8: - ETA: 7: - ETA: 7: - ETA: 6: - ETA: 6: - ETA: 5: - ETA: 5: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 3: - ETA: 3: - ETA: 2: - ETA: 2: - ETA: 1: - ETA: 1: - ETA: 53s - ETA: 26 - 667s 27s/step\n",
      "Found 500 images belonging to 1 classes.\n",
      "25/25 [==============================] - ETA: 22:4 - ETA: 21:4 - ETA: 20:2 - ETA: 19:0 - ETA: 18:2 - ETA: 17:4 - ETA: 16:4 - ETA: 15:3 - ETA: 14:4 - ETA: 13:4 - ETA: 12:5 - ETA: 12:0 - ETA: 11:0 - ETA: 10:0 - ETA: 9:1 - ETA: 8: - ETA: 7: - ETA: 6: - ETA: 5: - ETA: 4: - ETA: 3: - ETA: 2: - ETA: 1: - ETA: 55s - 1382s 55s/step\n"
     ]
    }
   ],
   "source": [
    "batch_extract_small_test_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(small_models_and_features) == 0:\n",
    "    small_models_and_features['vgg16'] = 'small_vgg16_features.npz'\n",
    "    small_models_and_features['vgg19'] = 'small_vgg19_features.npz'\n",
    "    small_models_and_features['resnet50'] = 'small_resnet50_features.npz'\n",
    "    small_models_and_features['inceptionV3'] = 'small_inception_v3_features.npz'\n",
    "    small_models_and_features['xception'] = 'small_xception_features.npz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(small_models_and_test_features) == 0:\n",
    "    small_models_and_test_features['vgg16'] = 'test_vgg16_features.npz'\n",
    "    small_models_and_test_features['vgg19'] = 'test_vgg19_features.npz'\n",
    "    small_models_and_test_features['resnet50'] = 'test_resnet50_features.npz'\n",
    "    small_models_and_test_features['inceptionV3'] = 'test_inception_v3_features.npz'\n",
    "    small_models_and_test_features['xception'] = 'test_xception_features.npz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_train(key, dropout, optimizer, file_header):\n",
    "    features = np.load(small_models_and_features[key])\n",
    "    features_test = np.load(small_models_and_test_features[key])\n",
    "    train_features = features['train']\n",
    "    train_labels = features['train_label']\n",
    "    valid_features = features['valid']\n",
    "    valid_labels = features['valid_label']\n",
    "    test_features = features_test['test']\n",
    "    test_filenames = features_test['test_filename']\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Dense(2048, input_shape=(train_features.shape[1],), activation='relu'))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Dense(2048, activation='relu'))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    checkpointer = ModelCheckpoint(filepath='{0}.hdf5'.format(file_header), verbose=1, save_best_only=True)\n",
    "    train_result = model.fit(train_features, train_labels, epochs=10, batch_size=common.batch_size,\n",
    "          validation_data=(valid_features, valid_labels), verbose=1, callbacks=[checkpointer])\n",
    "    \n",
    "    with open('{0}_history.pkl'.format(file_header), 'wb') as f:\n",
    "        pickle.dump(train_result.history, f)\n",
    "    model.load_weights('{0}.hdf5'.format(file_header))\n",
    "    prediction = model.predict(test_features, batch_size=common.batch_size)\n",
    "    prediction = prediction[:, 0].clip(0.01, 0.99)\n",
    "    test_fileindex = np.array([os.path.splitext(os.path.split(filename)[1])[0] for filename in test_filenames])\n",
    "    data = np.stack([test_fileindex, prediction], axis=1)\n",
    "    tmp = pd.DataFrame(data, columns=['id', 'label'])\n",
    "    tmp['id'] = tmp['id'].apply(pd.to_numeric)\n",
    "    submit_frame = pd.read_csv('dogs-vs-cats/sample_submission.csv')\n",
    "    result = pd.merge(submit_frame, tmp, on=\"id\", how='left')\n",
    "    result = result.rename(index=str, columns={\"label_y\": \"label\"})\n",
    "    result.dropna(axis=0, subset=['label'], inplace=True)\n",
    "    result[['id','label']].to_csv('{0}_predict.csv'.format(file_header),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_info(model_name, acc_lengend, loss_lengend):\n",
    "    adam_3 = 'small_{0}_Adam_0.3_history.pkl'.format(model_name)\n",
    "    adam_5 = 'small_{0}_Adam_0.5_history.pkl'.format(model_name)\n",
    "    adam_7 = 'small_{0}_Adam_0.7_history.pkl'.format(model_name)\n",
    "    sgd_3 = 'small_{0}_SGD_0.3_history.pkl'.format(model_name)\n",
    "    sgd_5 = 'small_{0}_SGD_0.5_history.pkl'.format(model_name)\n",
    "    sgd_7 = 'small_{0}_SGD_0.7_history.pkl'.format(model_name)\n",
    "    with open(adam_3, 'rb') as file:\n",
    "        history = pickle.load(file)\n",
    "        acc_adam_3 = history['acc']\n",
    "        loss_adam_3 = history['loss']\n",
    "        val_acc_adam_3 = history['val_acc']\n",
    "        val_loss_adam_3 = history['val_loss']\n",
    "    \n",
    "    with open(adam_5, 'rb') as file:\n",
    "        history = pickle.load(file)\n",
    "        acc_adam_5 = history['acc']\n",
    "        loss_adam_5 = history['loss']\n",
    "        val_acc_adam_5 = history['val_acc']\n",
    "        val_loss_adam_5 = history['val_loss']\n",
    "        \n",
    "    with open(adam_7, 'rb') as file:\n",
    "        history = pickle.load(file)\n",
    "        acc_adam_7 = history['acc']\n",
    "        loss_adam_7 = history['loss']\n",
    "        val_acc_adam_7 = history['val_acc']\n",
    "        val_loss_adam_7 = history['val_loss']\n",
    "        \n",
    "    with open(sgd_3, 'rb') as file:\n",
    "        history = pickle.load(file)\n",
    "        acc_sgd_3 = history['acc']\n",
    "        loss_sgd_3 = history['loss']\n",
    "        val_acc_sgd_3 = history['val_acc']\n",
    "        val_loss_sgd_3 = history['val_loss']\n",
    "        \n",
    "    with open(sgd_5, 'rb') as file:\n",
    "        history = pickle.load(file)\n",
    "        acc_sgd_5 = history['acc']\n",
    "        loss_sgd_5 = history['loss']\n",
    "        val_acc_sgd_5 = history['val_acc']\n",
    "        val_loss_sgd_5 = history['val_loss']\n",
    "        \n",
    "    with open(sgd_7, 'rb') as file:\n",
    "        history = pickle.load(file)\n",
    "        acc_sgd_7 = history['acc']\n",
    "        loss_sgd_7 = history['loss']\n",
    "        val_acc_sgd_7 = history['val_acc']\n",
    "        val_loss_sgd_7 = history['val_loss']\n",
    "    plt.figure(figsize=(16, 16))\n",
    "    plt.subplot(431)\n",
    "    plt.plot(range(1, len(acc_adam_3) + 1), acc_adam_3, label='train adam .3')\n",
    "    plt.plot(range(1, len(val_acc_adam_3) + 1), val_acc_adam_3, label='valid adam .3')\n",
    "    plt.legend(loc=acc_lengend, shadow=True, fontsize='x-large')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    \n",
    "    plt.subplot(432)\n",
    "    plt.plot(range(1, len(acc_adam_5) + 1), acc_adam_5, label='train adam .5')\n",
    "    plt.plot(range(1, len(val_acc_adam_5) + 1), val_acc_adam_5, label='valid adam .5')\n",
    "    plt.legend(loc=acc_lengend, shadow=True, fontsize='x-large')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    \n",
    "    plt.subplot(433)\n",
    "    plt.plot(range(1, len(acc_adam_7) + 1), acc_adam_7, label='train adam .7')\n",
    "    plt.plot(range(1, len(val_acc_adam_7) + 1), val_acc_adam_7, label='valid adam .7')\n",
    "    plt.legend(loc=acc_lengend, shadow=True, fontsize='x-large')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    \n",
    "    plt.subplot(434)\n",
    "    plt.plot(range(1, len(acc_sgd_3) + 1), acc_sgd_3, label='train sgd .3')\n",
    "    plt.plot(range(1, len(val_acc_sgd_3) + 1), val_acc_adam_3, label='valid sgd .3')\n",
    "    plt.legend(loc=acc_lengend, shadow=True, fontsize='x-large')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    \n",
    "    plt.subplot(435)\n",
    "    plt.plot(range(1, len(acc_sgd_5) + 1), acc_sgd_5, label='train sgd .5')\n",
    "    plt.plot(range(1, len(val_acc_sgd_5) + 1), val_acc_sgd_5, label='valid sgd .5')\n",
    "    plt.legend(loc=acc_lengend, shadow=True, fontsize='x-large')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    \n",
    "    plt.subplot(436)\n",
    "    plt.plot(range(1, len(acc_sgd_7) + 1), acc_sgd_7, label='train sgd .7')\n",
    "    plt.plot(range(1, len(val_acc_sgd_7) + 1), val_acc_sgd_7, label='valid sgd .7')\n",
    "    plt.xticks(range(1, len(acc_adam_3) + 1));\n",
    "    plt.legend(loc=acc_lengend, shadow=True, fontsize='x-large')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    \n",
    "    plt.subplot(437)\n",
    "    plt.plot(range(1, len(loss_adam_3) + 1), loss_adam_3, label='train adam .3')\n",
    "    plt.plot(range(1, len(val_loss_adam_3) + 1), val_loss_adam_3, label='valid adam .3')\n",
    "    plt.legend(loc=acc_lengend, shadow=True, fontsize='x-large')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('LogLoss')\n",
    "    plt.subplot(438)\n",
    "    plt.plot(range(1, len(loss_adam_5) + 1), loss_adam_5, label='train adam .5')\n",
    "    plt.plot(range(1, len(val_loss_adam_5) + 1), val_loss_adam_5, label='valid adam .5')\n",
    "    plt.legend(loc=loss_lengend, shadow=True, fontsize='x-large')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('LogLoss');\n",
    "    \n",
    "    plt.subplot(439)\n",
    "    plt.plot(range(1, len(loss_adam_7) + 1), loss_adam_7, label='train adam .7')\n",
    "    plt.plot(range(1, len(val_loss_adam_7) + 1), val_loss_adam_7, label='valid adam .7')\n",
    "    plt.legend(loc=loss_lengend, shadow=True, fontsize='x-large')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('LogLoss');\n",
    "    \n",
    "    plt.subplot(4, 3, 10)\n",
    "    plt.plot(range(1, len(loss_sgd_3) + 1), loss_sgd_3, label='train sgd .3')\n",
    "    plt.plot(range(1, len(val_loss_sgd_3) + 1), val_loss_adam_3, label='valid sgd .3')\n",
    "    plt.legend(loc=loss_lengend, shadow=True, fontsize='x-large')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('LogLoss');\n",
    "    \n",
    "    plt.subplot(4, 3, 11)\n",
    "    plt.plot(range(1, len(loss_sgd_5) + 1), loss_sgd_5, label='train sgd .5')\n",
    "    plt.plot(range(1, len(val_loss_sgd_5) + 1), val_loss_sgd_5, label='valid sgd .5')\n",
    "    plt.legend(loc=loss_lengend, shadow=True, fontsize='x-large')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('LogLoss');\n",
    "    \n",
    "    plt.subplot(4, 3, 12)\n",
    "    plt.plot(range(1, len(loss_sgd_7) + 1), loss_sgd_7, label='train sgd .7')\n",
    "    plt.plot(range(1, len(val_loss_sgd_7) + 1), val_loss_sgd_7, label='valid sgd .7')\n",
    "    plt.xticks(range(1, len(loss_adam_3) + 1));\n",
    "    plt.legend(loc=loss_lengend, shadow=True, fontsize='x-large')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('LogLoss');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_train('vgg16', 0.3, Adam(lr=0.001), 'small_vgg16_Adam_0.3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_train('vgg16', 0.5, Adam(lr=0.001), 'small_vgg16_Adam_0.5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_train('vgg16', 0.7, Adam(lr=0.001), 'small_vgg16_Adam_0.7')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_train('vgg16', 0.3, SGD(lr=0.001), 'small_vgg16_SGD_0.3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_train('vgg16', 0.5, SGD(lr=0.001), 'small_vgg16_SGD_0.5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_train('vgg16', 0.7, SGD(lr=0.001), 'small_vgg16_SGD_0.7')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VGG19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_train('vgg19', 0.3, Adam(lr=0.001), 'small_vgg19_Adam_0.3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_train('vgg19', 0.5, Adam(lr=0.001), 'small_vgg19_Adam_0.5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_train('vgg19', 0.7, Adam(lr=0.001), 'small_vgg19_Adam_0.7')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_train('vgg19', 0.3, SGD(lr=0.001), 'small_vgg19_SGD_0.3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_train('vgg19', 0.5, SGD(lr=0.001), 'small_vgg19_SGD_0.5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_train('vgg19', 0.7, SGD(lr=0.001), 'small_vgg19_SGD_0.7')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_train('resnet50', 0.3, Adam(lr=0.001), 'small_resnet50_Adam_0.3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_train('resnet50', 0.5, Adam(lr=0.001), 'small_resnet50_Adam_0.5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_train('resnet50', 0.7, Adam(lr=0.001), 'small_resnet50_Adam_0.7')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_train('resnet50', 0.3, SGD(lr=0.001), 'small_resnet50_SGD_0.3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_train('resnet50', 0.5, SGD(lr=0.001), 'small_resnet50_SGD_0.5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_train('resnet50', 0.7, SGD(lr=0.001), 'small_resnet50_SGD_0.7')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## InceptionV3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_train('inceptionV3', 0.3, Adam(lr=0.001), 'small_inceptionV3_Adam_0.3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_train('inceptionV3', 0.5, Adam(lr=0.001), 'small_inceptionV3_Adam_0.5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_train('inceptionV3', 0.7, Adam(lr=0.001), 'small_inceptionV3_Adam_0.7')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_train('inceptionV3', 0.3, SGD(lr=0.001), 'small_inceptionV3_SGD_0.3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_train('inceptionV3', 0.5, SGD(lr=0.001), 'small_inceptionV3_SGD_0.5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_train('inceptionV3', 0.7, SGD(lr=0.001), 'small_inceptionV3_SGD_0.7')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Xception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_train('xception', 0.3, Adam(lr=0.001), 'small_xception_Adam_0.3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_train('xception', 0.5, Adam(lr=0.001), 'small_xception_Adam_0.5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_train('xception', 0.5, Adam(lr=0.001), 'small_xception_Adam_0.5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_train('xception', 0.3, SGD(lr=0.001), 'small_xception_SGD_0.3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_train('xception', 0.5, SGD(lr=0.001), 'small_xception_SGD_0.5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_train('xception', 0.7, SGD(lr=0.001), 'small_xception_SGD_0.7')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
