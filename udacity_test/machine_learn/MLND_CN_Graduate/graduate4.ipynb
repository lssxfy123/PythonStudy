{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 自己构建的网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_files       \n",
    "from keras.utils import np_utils\n",
    "from keras.layers import Lambda\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras.applications.resnet50 import preprocess_input as resnet50_pre\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.applications.inception_v3 import preprocess_input as inceptionV3_pre\n",
    "from keras.applications.xception import Xception\n",
    "from keras.applications.xception import preprocess_input as xception_pre\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.applications.vgg16 import preprocess_input as vgg16_pre\n",
    "from keras.applications.vgg19 import VGG19\n",
    "from keras.applications.vgg19 import preprocess_input as vgg19_pre\n",
    "from keras.preprocessing import image   \n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from tqdm import tqdm\n",
    "from PIL import ImageFile  \n",
    "from keras.applications.resnet50 import preprocess_input, decode_predictions\n",
    "from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.layers import Dropout, Flatten, Dense, Input, Activation\n",
    "from keras.models import Sequential, Model\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import ModelCheckpoint  \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "import cv2                \n",
    "import matplotlib.pyplot as plt    \n",
    "import matplotlib.image as mpimg\n",
    "%matplotlib inline \n",
    "import random\n",
    "import os\n",
    "import shutil\n",
    "import h5py\n",
    "import common\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 对图片进行分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "common.divide_images()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_and_features = {}\n",
    "models_and_test_features = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batch_extract_features():\n",
    "    \n",
    "    # VGG16\n",
    "    base_model = VGG16(weights='imagenet', include_top=False, pooling='avg')\n",
    "    features_name = common.extract_features(base_model, (224, 224), vgg16_pre)\n",
    "    models_and_features['vgg16'] = features_name\n",
    "    \n",
    "    # VGG19\n",
    "    base_model = VGG19(weights='imagenet', include_top=False, pooling='avg')\n",
    "    features_name = common.extract_features(base_model, (224, 224), vgg19_pre)\n",
    "    models_and_features['vgg19'] = features_name\n",
    "    \n",
    "    # ResNet50\n",
    "    base_model = ResNet50(weights='imagenet', include_top=False, pooling='avg')\n",
    "    features_name = common.extract_features(base_model, (224, 224), resnet50_pre)\n",
    "    models_and_features['resnet50'] = features_name\n",
    "    \n",
    "    # InceptionV3\n",
    "    base_model = InceptionV3(weights='imagenet', include_top=False, pooling='avg')\n",
    "    features_name = common.extract_features(base_model, (299, 299), inceptionV3_pre)\n",
    "    models_and_features['inceptionV3'] = features_name\n",
    "    \n",
    "    # Xception\n",
    "    base_model = Xception(weights='imagenet', include_top=False, pooling='avg')\n",
    "    features_name = common.extract_features(base_model, (299, 299), xception_pre)\n",
    "    models_and_features['xception'] = features_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_extract_test_features():\n",
    "    \n",
    "    # VGG16\n",
    "    base_model = VGG16(weights='imagenet', include_top=False, pooling='avg')\n",
    "    features_name = common.extract_test_features(base_model, (224, 224), vgg16_pre)\n",
    "    models_and_test_features['vgg16'] = features_name\n",
    "    \n",
    "    # VGG19\n",
    "    base_model = VGG19(weights='imagenet', include_top=False, pooling='avg')\n",
    "    features_name = common.extract_test_features(base_model, (224, 224), vgg19_pre)\n",
    "    models_and_test_features['vgg19'] = features_name\n",
    "    \n",
    "    # ResNet50\n",
    "    base_model = ResNet50(weights='imagenet', include_top=False, pooling='avg')\n",
    "    features_name = common.extract_test_features(base_model, (224, 224), resnet50_pre)\n",
    "    models_and_test_features['resnet50'] = features_name\n",
    "    \n",
    "    # InceptionV3\n",
    "    base_model = InceptionV3(weights='imagenet', include_top=False, pooling='avg')\n",
    "    features_name = common.extract_test_features(base_model, (299, 299), inceptionV3_pre)\n",
    "    models_and_test_features['inceptionV3'] = features_name\n",
    "    \n",
    "    # Xception\n",
    "    base_model = Xception(weights='imagenet', include_top=False, pooling='avg')\n",
    "    features_name = common.extract_test_features(base_model, (299, 299), xception_pre)\n",
    "    models_and_test_features['xception'] = features_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_extract_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_extract_test_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(models_and_features) == 0:\n",
    "    models_and_features['vgg16'] = 'vgg16_features.npz'\n",
    "    models_and_features['vgg19'] = 'vgg19_features.npz'\n",
    "    models_and_features['resnet50'] = 'resnet50_features.npz'\n",
    "    models_and_features['inceptionV3'] = 'inception_v3_features.npz'\n",
    "    models_and_features['xception'] = 'xception_features.npz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(models_and_test_features) == 0:\n",
    "    models_and_test_features['vgg16'] = 'test_vgg16_features.npz'\n",
    "    models_and_test_features['vgg19'] = 'test_vgg19_features.npz'\n",
    "    models_and_test_features['resnet50'] = 'test_resnet50_features.npz'\n",
    "    models_and_test_features['inceptionV3'] = 'test_inception_v3_features.npz'\n",
    "    models_and_test_features['xception'] = 'test_xception_features.npz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 13s 631us/step - loss: 1.0899 - acc: 0.9286 - val_loss: 0.7611 - val_acc: 0.9524\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.76108, saving model to vgg16.hdf5\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 11s 546us/step - loss: 1.6651 - acc: 0.8956 - val_loss: 1.0165 - val_acc: 0.9360\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.76108\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 11s 547us/step - loss: 1.1020 - acc: 0.9312 - val_loss: 0.4550 - val_acc: 0.9716\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.76108 to 0.45505, saving model to vgg16.hdf5\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 11s 548us/step - loss: 1.5155 - acc: 0.9055 - val_loss: 1.2992 - val_acc: 0.9192\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.45505\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 11s 546us/step - loss: 1.3466 - acc: 0.9159 - val_loss: 0.3981 - val_acc: 0.9752\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.45505 to 0.39815, saving model to vgg16.hdf5\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 11s 548us/step - loss: 0.9713 - acc: 0.9393 - val_loss: 3.0332 - val_acc: 0.8116\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.39815\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 11s 547us/step - loss: 2.2741 - acc: 0.8587 - val_loss: 1.9210 - val_acc: 0.8808\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.39815\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 11s 546us/step - loss: 1.2605 - acc: 0.9212 - val_loss: 0.7302 - val_acc: 0.9540\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.39815\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 11s 548us/step - loss: 0.9093 - acc: 0.9430 - val_loss: 0.6770 - val_acc: 0.9576\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.39815\n",
      "Epoch 10/20\n",
      "20000/20000 [==============================] - 11s 546us/step - loss: 0.8203 - acc: 0.9486 - val_loss: 0.5541 - val_acc: 0.9652\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.39815\n",
      "Epoch 11/20\n",
      "20000/20000 [==============================] - 11s 547us/step - loss: 1.2328 - acc: 0.9227 - val_loss: 0.4415 - val_acc: 0.9724\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.39815\n",
      "Epoch 12/20\n",
      "20000/20000 [==============================] - 11s 547us/step - loss: 0.6651 - acc: 0.9584 - val_loss: 0.4070 - val_acc: 0.9746\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.39815\n",
      "Epoch 13/20\n",
      "20000/20000 [==============================] - 11s 547us/step - loss: 2.7314 - acc: 0.8304 - val_loss: 0.4131 - val_acc: 0.9742\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.39815\n",
      "Epoch 14/20\n",
      "20000/20000 [==============================] - 11s 546us/step - loss: 0.6163 - acc: 0.9615 - val_loss: 0.4131 - val_acc: 0.9742\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.39815\n",
      "Epoch 15/20\n",
      "20000/20000 [==============================] - 11s 546us/step - loss: 1.2838 - acc: 0.9196 - val_loss: 6.0230 - val_acc: 0.6222\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.39815\n",
      "Epoch 16/20\n",
      "20000/20000 [==============================] - 11s 548us/step - loss: 2.0445 - acc: 0.8718 - val_loss: 4.3209 - val_acc: 0.7288\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.39815\n",
      "Epoch 17/20\n",
      "20000/20000 [==============================] - 11s 548us/step - loss: 1.2693 - acc: 0.9209 - val_loss: 0.5648 - val_acc: 0.9648\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.39815\n",
      "Epoch 18/20\n",
      "19080/20000 [===========================>..] - ETA: 0s - loss: 0.7217 - acc: 0.9551"
     ]
    }
   ],
   "source": [
    "# 训练+预测\n",
    "for key, value in models_and_features.items():\n",
    "    print('Train Model {0}'.format(key))\n",
    "    features = np.load(value)\n",
    "    features_test = np.load(models_and_test_features[key])\n",
    "    train_features = features['train']\n",
    "    train_labels = features['train_label']\n",
    "    valid_features = features['valid']\n",
    "    valid_labels = features['valid_label']\n",
    "    test_features = features_test['test']\n",
    "    test_filenames = features_test['test_filename']\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Dense(2048, input_shape=(train_features.shape[1],), activation='relu'))\n",
    "    model.add(Dropout(.5))\n",
    "    model.add(Dense(2048, activation='relu'))\n",
    "    model.add(Dropout(.5))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(Adam(lr=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    checkpointer = ModelCheckpoint(filepath='{0}.hdf5'.format(key), verbose=1, save_best_only=True)\n",
    "    train_result = model.fit(train_features, train_labels, epochs=20, batch_size=common.batch_size,\n",
    "          validation_data=(valid_features, valid_labels), verbose=1, callbacks=[checkpointer])\n",
    "    \n",
    "    with open('{0}_history.pkl'.format(key), 'wb') as f:\n",
    "        pickle.dump(train_result.history, f)\n",
    "    model.load_weights('{0}.hdf5'.format(key))\n",
    "    prediction = model.predict(test_features, batch_size=common.batch_size)\n",
    "    prediction = prediction[:, 0].clip(0.01, 0.99)\n",
    "    test_fileindex = np.array([os.path.splitext(os.path.split(filename)[1])[0] for filename in test_filenames])\n",
    "    data = np.stack([test_fileindex, prediction], axis=1)\n",
    "    tmp = pd.DataFrame(data, columns=['id', 'label'])\n",
    "    tmp['id'] = tmp['id'].apply(pd.to_numeric)\n",
    "    submit_frame = pd.read_csv('dogs-vs-cats/sample_submission.csv')\n",
    "    result = pd.merge(submit_frame, tmp, on=\"id\", how='left')\n",
    "    result = result.rename(index=str, columns={\"label_y\": \"label\"})\n",
    "    result[['id','label']].to_csv('{0}_predict.csv'.format(key),index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 预测\n",
    "for key, value in models_and_test_features.items():\n",
    "    features_test = np.load(value)\n",
    "    test_features = features_test['test']\n",
    "    test_filenames = features_test['test_filename']\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Dense(2048, input_shape=(train_features.shape[1],), activation='relu'))\n",
    "    model.add(Dropout(.5))\n",
    "    model.add(Dense(2048, activation='relu'))\n",
    "    model.add(Dropout(.5))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(Adam(lr=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    model.load_weights('{0}.hdf5'.format(key))\n",
    "    prediction = model.predict(test_features, batch_size=common.batch_size)\n",
    "    prediction = prediction[:, 0].clip(0.01, 0.99)\n",
    "    test_fileindex = np.array([os.path.splitext(os.path.split(filename)[1])[0] for filename in test_filenames])\n",
    "    data = np.stack([test_fileindex, prediction], axis=1)\n",
    "    tmp = pd.DataFrame(data, columns=['id', 'label'])\n",
    "    tmp['id'] = tmp['id'].apply(pd.to_numeric)\n",
    "    submit_frame = pd.read_csv('dogs-vs-cats/sample_submission.csv')\n",
    "    result = pd.merge(submit_frame, tmp, on=\"id\", how='left')\n",
    "    result = result.rename(index=str, columns={\"label_y\": \"label\"})\n",
    "    result[['id','label']].to_csv('{0}_predict.csv'.format(key),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
