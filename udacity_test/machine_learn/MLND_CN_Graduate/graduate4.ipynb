{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 自己构建的网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_files       \n",
    "from keras.utils import np_utils\n",
    "from keras.layers import Lambda\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras.applications.resnet50 import preprocess_input as resnet50_pre\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.applications.inception_v3 import preprocess_input as inceptionV3_pre\n",
    "from keras.applications.xception import Xception\n",
    "from keras.applications.xception import preprocess_input as xception_pre\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.applications.vgg16 import preprocess_input as vgg16_pre\n",
    "from keras.applications.vgg19 import VGG19\n",
    "from keras.applications.vgg19 import preprocess_input as vgg19_pre\n",
    "from keras.preprocessing import image   \n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from tqdm import tqdm\n",
    "from PIL import ImageFile  \n",
    "from keras.applications.resnet50 import preprocess_input, decode_predictions\n",
    "from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.layers import Dropout, Flatten, Dense, Input, Activation\n",
    "from keras.models import Sequential, Model\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import ModelCheckpoint  \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "import cv2                \n",
    "import matplotlib.pyplot as plt    \n",
    "import matplotlib.image as mpimg\n",
    "%matplotlib inline \n",
    "import random\n",
    "import os\n",
    "import shutil\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def move_data(names, divide_train_path, divide_valid_path):\n",
    "    os.mkdir(divide_train_path)\n",
    "    os.mkdir(divide_valid_path)      \n",
    "    valid_names = random.sample(names, int(len(names) * 0.2))\n",
    "    train_names = [name for name in names if name not in valid_names]\n",
    "    [shutil.move(train_path + name, divide_train_path + name) for name in train_names]\n",
    "    [shutil.move(train_path + name, divide_valid_path + name) for name in valid_names]\n",
    "\n",
    "    \n",
    "def divide_images():\n",
    "    # 训练集\n",
    "    train_dog_path = train_path + 'dog/'\n",
    "    train_cat_path = train_path + 'cat/'\n",
    "    valid_dog_path = valid_path + 'dog/'\n",
    "    valid_cat_path = valid_path + 'cat/'\n",
    "    \n",
    "    if not os.path.exists(train_dog_path):\n",
    "        names= os.listdir(train_path)\n",
    "        cat_names = [name for name in names if name.startswith('cat')]\n",
    "        dog_names = [name for name in names if name.startswith('dog')]\n",
    "        move_data(cat_names, train_cat_path, valid_cat_path)\n",
    "        move_data(dog_names, train_dog_path, valid_dog_path)\n",
    "    \n",
    "    divide_test_path = test_path + 'test/'\n",
    "    if not os.path.exists(divide_test_path):\n",
    "        names = os.listdir(test_path)\n",
    "        os.mkdir(divide_test_path)\n",
    "        [shutil.move(test_path + name, divide_test_path + name) for name in names]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 对图片进行分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = 'dogs-vs-cats/train1/'\n",
    "test_path = 'dogs-vs-cats/test1/'\n",
    "valid_path = 'dogs-vs-cats/valid1/'\n",
    "if not os.path.exists(valid_path):\n",
    "    os.mkdir(valid_path)\n",
    "    \n",
    "divide_images()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features1(base_model, target_size, preprocess):\n",
    "    datagen = ImageDataGenerator(preprocessing_function=preprocess)\n",
    "#     valid_datagen = ImageDataGenerator(rescale=1./255)\n",
    "#     test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "    \n",
    "    train_generator = datagen.flow_from_directory(train_path, target_size=target_size,\n",
    "                                                    batch_size=batch_size, class_mode='binary', shuffle=False)\n",
    "    valid_generator = datagen.flow_from_directory(valid_path, target_size=target_size,\n",
    "                                                    batch_size=batch_size, class_mode='binary', shuffle=False)\n",
    "    test_generator = datagen.flow_from_directory(test_path, target_size=target_size, batch_size=batch_size, class_mode=None, shuffle=True)\n",
    "    train_features = base_model.predict_generator(train_generator, train_generator.samples // batch_size)\n",
    "    valid_features = base_model.predict_generator(valid_generator, valid_generator.samples // batch_size)\n",
    "    test_featrues = base_model.predict_generator(test_generator, test_generator.samples // batch_size)\n",
    "    \n",
    "    np.savez('{0}_features.npz'.format(base_model.name),train=train_features, train_label=train_generator.classes,\n",
    "             valid=valid_features, valid_label=valid_generator.classes, test=test_featrues, test_filename=test_generator.filenames)\n",
    "\n",
    "\n",
    "batch_size = 20\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(base_model, target_size):\n",
    "    datagen = ImageDataGenerator(rescale=1./255)\n",
    "#     valid_datagen = ImageDataGenerator(rescale=1./255)\n",
    "#     test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "    \n",
    "    train_generator = datagen.flow_from_directory(train_path, target_size=target_size,\n",
    "                                                    batch_size=batch_size, class_mode='binary', shuffle=False)\n",
    "    valid_generator = datagen.flow_from_directory(valid_path, target_size=target_size,\n",
    "                                                    batch_size=batch_size, class_mode='binary', shuffle=False)\n",
    "    test_generator = datagen.flow_from_directory(test_path, target_size=target_size, batch_size=batch_size, class_mode=None)\n",
    "    train_features = base_model.predict_generator(train_generator, train_generator.samples // batch_size)\n",
    "    valid_features = base_model.predict_generator(valid_generator, valid_generator.samples // batch_size)\n",
    "    test_featrues = base_model.predict_generator(test_generator, test_generator.samples // batch_size)\n",
    "    \n",
    "    np.savez('{0}_features.npz'.format(base_model.name),train=train_features, train_label=train_generator.classes,\n",
    "             valid=valid_features, valid_label=valid_generator.classes, test=test_featrues)\n",
    "\n",
    "\n",
    "batch_size = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = ResNet50(weights='imagenet', include_top=False, pooling='avg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 2 classes.\n",
      "Found 800 images belonging to 2 classes.\n",
      "Found 100 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "extract_features1(base_model, (224, 224), resnet50_pre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet50_features = np.load('resnet50_features.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = resnet50_features['train']\n",
    "train_labels = resnet50_features['train_label']\n",
    "valid_features = resnet50_features['valid']\n",
    "valid_labels = resnet50_features['valid_label']\n",
    "test_features = resnet50_features['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "#model.add(Flatten(input_shape=(train_features.shape[1],)))\n",
    "model.add(Dense(2048, input_shape=(train_features.shape[1],), activation='relu'))\n",
    "model.add(Dropout(.5))\n",
    "model.add(Dense(2048, activation='relu'))\n",
    "model.add(Dropout(.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(Adam(lr=0.001), loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2000 samples, validate on 800 samples\n",
      "Epoch 1/2\n",
      "2000/2000 [==============================] - ETA: 6:38 - loss: 0.8328 - acc: 0.450 - ETA: 3:24 - loss: 1.6082 - acc: 0.525 - ETA: 2:19 - loss: 1.6170 - acc: 0.516 - ETA: 1:47 - loss: 1.2433 - acc: 0.625 - ETA: 1:27 - loss: 1.1509 - acc: 0.660 - ETA: 1:14 - loss: 0.9845 - acc: 0.708 - ETA: 1:05 - loss: 0.8901 - acc: 0.735 - ETA: 58s - loss: 0.8298 - acc: 0.743 - ETA: 52s - loss: 0.7535 - acc: 0.76 - ETA: 48s - loss: 0.7397 - acc: 0.78 - ETA: 44s - loss: 0.6807 - acc: 0.80 - ETA: 41s - loss: 0.6703 - acc: 0.81 - ETA: 38s - loss: 0.6971 - acc: 0.81 - ETA: 36s - loss: 0.6476 - acc: 0.83 - ETA: 34s - loss: 0.7006 - acc: 0.82 - ETA: 33s - loss: 0.6575 - acc: 0.83 - ETA: 31s - loss: 0.6303 - acc: 0.84 - ETA: 30s - loss: 0.5953 - acc: 0.85 - ETA: 28s - loss: 0.6485 - acc: 0.84 - ETA: 27s - loss: 0.6631 - acc: 0.85 - ETA: 26s - loss: 0.6316 - acc: 0.85 - ETA: 25s - loss: 0.6030 - acc: 0.86 - ETA: 24s - loss: 0.6421 - acc: 0.86 - ETA: 23s - loss: 0.6933 - acc: 0.86 - ETA: 23s - loss: 0.6739 - acc: 0.86 - ETA: 22s - loss: 0.6512 - acc: 0.86 - ETA: 21s - loss: 0.6271 - acc: 0.87 - ETA: 20s - loss: 0.6105 - acc: 0.87 - ETA: 20s - loss: 0.5897 - acc: 0.87 - ETA: 19s - loss: 0.6338 - acc: 0.87 - ETA: 19s - loss: 0.6137 - acc: 0.87 - ETA: 18s - loss: 0.6195 - acc: 0.88 - ETA: 18s - loss: 0.6007 - acc: 0.88 - ETA: 17s - loss: 0.5982 - acc: 0.88 - ETA: 17s - loss: 0.5811 - acc: 0.88 - ETA: 16s - loss: 0.5650 - acc: 0.89 - ETA: 16s - loss: 0.5497 - acc: 0.89 - ETA: 15s - loss: 0.5421 - acc: 0.89 - ETA: 15s - loss: 0.5285 - acc: 0.89 - ETA: 14s - loss: 0.5257 - acc: 0.89 - ETA: 14s - loss: 0.5379 - acc: 0.89 - ETA: 14s - loss: 0.5250 - acc: 0.90 - ETA: 13s - loss: 0.5129 - acc: 0.90 - ETA: 13s - loss: 0.5033 - acc: 0.90 - ETA: 13s - loss: 0.4993 - acc: 0.90 - ETA: 12s - loss: 0.5047 - acc: 0.90 - ETA: 12s - loss: 0.5364 - acc: 0.90 - ETA: 12s - loss: 0.5703 - acc: 0.90 - ETA: 11s - loss: 0.5749 - acc: 0.90 - ETA: 11s - loss: 0.5741 - acc: 0.90 - ETA: 11s - loss: 0.5721 - acc: 0.90 - ETA: 10s - loss: 0.5792 - acc: 0.90 - ETA: 10s - loss: 0.5733 - acc: 0.90 - ETA: 10s - loss: 0.5649 - acc: 0.90 - ETA: 10s - loss: 0.5757 - acc: 0.90 - ETA: 9s - loss: 0.5654 - acc: 0.9071 - ETA: 9s - loss: 0.5555 - acc: 0.908 - ETA: 9s - loss: 0.5476 - acc: 0.909 - ETA: 8s - loss: 0.5518 - acc: 0.910 - ETA: 8s - loss: 0.5426 - acc: 0.911 - ETA: 8s - loss: 0.5392 - acc: 0.912 - ETA: 8s - loss: 0.5384 - acc: 0.912 - ETA: 7s - loss: 0.5302 - acc: 0.914 - ETA: 7s - loss: 0.5220 - acc: 0.915 - ETA: 7s - loss: 0.5140 - acc: 0.916 - ETA: 7s - loss: 0.5186 - acc: 0.917 - ETA: 6s - loss: 0.5264 - acc: 0.917 - ETA: 6s - loss: 0.5305 - acc: 0.917 - ETA: 6s - loss: 0.5345 - acc: 0.918 - ETA: 6s - loss: 0.5271 - acc: 0.919 - ETA: 5s - loss: 0.5225 - acc: 0.919 - ETA: 5s - loss: 0.5152 - acc: 0.920 - ETA: 5s - loss: 0.5082 - acc: 0.921 - ETA: 5s - loss: 0.5023 - acc: 0.922 - ETA: 5s - loss: 0.5010 - acc: 0.922 - ETA: 4s - loss: 0.4944 - acc: 0.923 - ETA: 4s - loss: 0.4879 - acc: 0.924 - ETA: 4s - loss: 0.4817 - acc: 0.925 - ETA: 4s - loss: 0.4985 - acc: 0.924 - ETA: 4s - loss: 0.4963 - acc: 0.925 - ETA: 3s - loss: 0.5000 - acc: 0.925 - ETA: 3s - loss: 0.4939 - acc: 0.926 - ETA: 3s - loss: 0.4976 - acc: 0.926 - ETA: 3s - loss: 0.4957 - acc: 0.926 - ETA: 2s - loss: 0.4898 - acc: 0.927 - ETA: 2s - loss: 0.4871 - acc: 0.927 - ETA: 2s - loss: 0.4815 - acc: 0.928 - ETA: 2s - loss: 0.4761 - acc: 0.929 - ETA: 2s - loss: 0.4707 - acc: 0.930 - ETA: 1s - loss: 0.4655 - acc: 0.931 - ETA: 1s - loss: 0.4636 - acc: 0.931 - ETA: 1s - loss: 0.4586 - acc: 0.932 - ETA: 1s - loss: 0.4537 - acc: 0.932 - ETA: 1s - loss: 0.4489 - acc: 0.933 - ETA: 0s - loss: 0.4442 - acc: 0.934 - ETA: 0s - loss: 0.4410 - acc: 0.934 - ETA: 0s - loss: 0.4364 - acc: 0.935 - ETA: 0s - loss: 0.4320 - acc: 0.935 - ETA: 0s - loss: 0.4276 - acc: 0.936 - 20s 10ms/step - loss: 0.4233 - acc: 0.9370 - val_loss: 0.1998 - val_acc: 0.9775\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.19977, saving model to resnet50.hdf5\n",
      "Epoch 2/2\n",
      "2000/2000 [==============================] - ETA: 14s - loss: 0.0745 - acc: 0.95 - ETA: 14s - loss: 0.0506 - acc: 0.97 - ETA: 14s - loss: 0.0393 - acc: 0.98 - ETA: 14s - loss: 0.0295 - acc: 0.98 - ETA: 14s - loss: 0.0236 - acc: 0.99 - ETA: 14s - loss: 0.0196 - acc: 0.99 - ETA: 14s - loss: 0.3585 - acc: 0.97 - ETA: 14s - loss: 0.3137 - acc: 0.97 - ETA: 14s - loss: 0.3674 - acc: 0.97 - ETA: 13s - loss: 0.4901 - acc: 0.96 - ETA: 13s - loss: 0.4455 - acc: 0.96 - ETA: 13s - loss: 0.4191 - acc: 0.96 - ETA: 13s - loss: 0.4482 - acc: 0.96 - ETA: 13s - loss: 0.4238 - acc: 0.96 - ETA: 13s - loss: 0.4074 - acc: 0.96 - ETA: 13s - loss: 0.3819 - acc: 0.96 - ETA: 13s - loss: 0.3594 - acc: 0.96 - ETA: 12s - loss: 0.3395 - acc: 0.96 - ETA: 12s - loss: 0.3217 - acc: 0.97 - ETA: 12s - loss: 0.3366 - acc: 0.97 - ETA: 12s - loss: 0.3206 - acc: 0.97 - ETA: 12s - loss: 0.3192 - acc: 0.97 - ETA: 12s - loss: 0.3430 - acc: 0.96 - ETA: 12s - loss: 0.3365 - acc: 0.96 - ETA: 11s - loss: 0.3230 - acc: 0.96 - ETA: 11s - loss: 0.3240 - acc: 0.96 - ETA: 11s - loss: 0.3415 - acc: 0.96 - ETA: 11s - loss: 0.3578 - acc: 0.96 - ETA: 11s - loss: 0.4137 - acc: 0.96 - ETA: 11s - loss: 0.4077 - acc: 0.96 - ETA: 10s - loss: 0.4237 - acc: 0.95 - ETA: 10s - loss: 0.4354 - acc: 0.95 - ETA: 10s - loss: 0.4309 - acc: 0.95 - ETA: 10s - loss: 0.4353 - acc: 0.95 - ETA: 10s - loss: 0.4459 - acc: 0.95 - ETA: 10s - loss: 0.4453 - acc: 0.95 - ETA: 9s - loss: 0.4550 - acc: 0.9554 - ETA: 9s - loss: 0.4902 - acc: 0.952 - ETA: 9s - loss: 0.4776 - acc: 0.953 - ETA: 9s - loss: 0.4657 - acc: 0.955 - ETA: 9s - loss: 0.4716 - acc: 0.954 - ETA: 9s - loss: 0.4793 - acc: 0.954 - ETA: 8s - loss: 0.4801 - acc: 0.954 - ETA: 8s - loss: 0.4692 - acc: 0.955 - ETA: 8s - loss: 0.4601 - acc: 0.955 - ETA: 8s - loss: 0.4674 - acc: 0.955 - ETA: 8s - loss: 0.4744 - acc: 0.955 - ETA: 8s - loss: 0.4646 - acc: 0.956 - ETA: 7s - loss: 0.4660 - acc: 0.956 - ETA: 7s - loss: 0.4683 - acc: 0.956 - ETA: 7s - loss: 0.4747 - acc: 0.955 - ETA: 7s - loss: 0.4656 - acc: 0.956 - ETA: 7s - loss: 0.4568 - acc: 0.957 - ETA: 7s - loss: 0.4678 - acc: 0.956 - ETA: 7s - loss: 0.4593 - acc: 0.957 - ETA: 6s - loss: 0.4511 - acc: 0.958 - ETA: 6s - loss: 0.4574 - acc: 0.957 - ETA: 6s - loss: 0.4727 - acc: 0.956 - ETA: 6s - loss: 0.4647 - acc: 0.957 - ETA: 6s - loss: 0.4704 - acc: 0.957 - ETA: 6s - loss: 0.4891 - acc: 0.956 - ETA: 5s - loss: 0.4824 - acc: 0.956 - ETA: 5s - loss: 0.4878 - acc: 0.955 - ETA: 5s - loss: 0.4851 - acc: 0.955 - ETA: 5s - loss: 0.4925 - acc: 0.954 - ETA: 5s - loss: 0.4913 - acc: 0.954 - ETA: 5s - loss: 0.4840 - acc: 0.955 - ETA: 4s - loss: 0.4769 - acc: 0.955 - ETA: 4s - loss: 0.4700 - acc: 0.956 - ETA: 4s - loss: 0.4936 - acc: 0.955 - ETA: 4s - loss: 0.4933 - acc: 0.954 - ETA: 4s - loss: 0.4975 - acc: 0.954 - ETA: 4s - loss: 0.5125 - acc: 0.954 - ETA: 4s - loss: 0.5165 - acc: 0.954 - ETA: 3s - loss: 0.5096 - acc: 0.954 - ETA: 3s - loss: 0.5029 - acc: 0.955 - ETA: 3s - loss: 0.4964 - acc: 0.955 - ETA: 3s - loss: 0.5002 - acc: 0.955 - ETA: 3s - loss: 0.4939 - acc: 0.956 - ETA: 3s - loss: 0.4977 - acc: 0.956 - ETA: 2s - loss: 0.4917 - acc: 0.956 - ETA: 2s - loss: 0.4857 - acc: 0.957 - ETA: 2s - loss: 0.4799 - acc: 0.957 - ETA: 2s - loss: 0.4741 - acc: 0.958 - ETA: 2s - loss: 0.4767 - acc: 0.958 - ETA: 2s - loss: 0.4712 - acc: 0.958 - ETA: 2s - loss: 0.4658 - acc: 0.959 - ETA: 1s - loss: 0.4651 - acc: 0.959 - ETA: 1s - loss: 0.4646 - acc: 0.959 - ETA: 1s - loss: 0.4683 - acc: 0.958 - ETA: 1s - loss: 0.4842 - acc: 0.957 - ETA: 1s - loss: 0.4789 - acc: 0.958 - ETA: 1s - loss: 0.4738 - acc: 0.958 - ETA: 0s - loss: 0.4687 - acc: 0.959 - ETA: 0s - loss: 0.4638 - acc: 0.959 - ETA: 0s - loss: 0.4590 - acc: 0.959 - ETA: 0s - loss: 0.4625 - acc: 0.959 - ETA: 0s - loss: 0.4666 - acc: 0.959 - ETA: 0s - loss: 0.4655 - acc: 0.959 - 16s 8ms/step - loss: 0.4610 - acc: 0.9595 - val_loss: 0.2364 - val_acc: 0.9850\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.19977\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1de576ce6d8>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpointer = ModelCheckpoint(filepath='resnet50.hdf5', verbose=1, save_best_only=True)\n",
    "\n",
    "model.fit(train_features, train_labels, epochs=2, batch_size=batch_size,\n",
    "          validation_data=(valid_features, valid_labels), verbose=1, callbacks=[checkpointer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model.predict(test_features, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = prediction[:, 0].clip(0.01, 0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_filenames = resnet50_features['test_filename']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_fileindex = np.array([os.path.splitext(os.path.split(filename)[1])[0] for filename in test_filenames])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.stack([test_fileindex, prediction], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1= pd.DataFrame(data, columns=['id', 'label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1.to_csv('resnet50.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = VGG16(weights='imagenet', include_top=False, pooling='avg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 2 classes.\n",
      "Found 800 images belonging to 2 classes.\n",
      "Found 100 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "extract_features1(base_model, (224, 224), vgg16_pre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg16_features = np.load('vgg16_features.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(512,)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vgg16_features['train'][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = vgg16_features['train']\n",
    "train_labels = vgg16_features['train_label']\n",
    "valid_features = vgg16_features['valid']\n",
    "valid_labels = vgg16_features['valid_label']\n",
    "test_features = vgg16_features['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "#model.add(Flatten(input_shape=train_features.shape[1:]))\n",
    "model.add(Dense(512, input_shape=(train_features.shape[1],), activation='relu'))\n",
    "model.add(Dropout(.5))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer=Adam(lr=0.001), loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2000 samples, validate on 800 samples\n",
      "Epoch 1/2\n",
      "2000/2000 [==============================] - ETA: 2:43 - loss: 3.3582 - acc: 0.400 - ETA: 40s - loss: 2.1629 - acc: 0.625 - ETA: 17s - loss: 2.2279 - acc: 0.66 - ETA: 12s - loss: 1.9566 - acc: 0.70 - ETA: 8s - loss: 1.4963 - acc: 0.7667 - ETA: 6s - loss: 1.3949 - acc: 0.791 - ETA: 5s - loss: 1.2481 - acc: 0.811 - ETA: 4s - loss: 1.1321 - acc: 0.829 - ETA: 3s - loss: 1.0179 - acc: 0.842 - ETA: 3s - loss: 0.9351 - acc: 0.851 - ETA: 2s - loss: 0.8657 - acc: 0.862 - ETA: 2s - loss: 0.8566 - acc: 0.866 - ETA: 2s - loss: 0.8051 - acc: 0.874 - ETA: 1s - loss: 0.7569 - acc: 0.881 - ETA: 1s - loss: 0.7252 - acc: 0.886 - ETA: 1s - loss: 0.7249 - acc: 0.886 - ETA: 1s - loss: 0.7002 - acc: 0.892 - ETA: 0s - loss: 0.6569 - acc: 0.899 - ETA: 0s - loss: 0.6498 - acc: 0.901 - ETA: 0s - loss: 0.6367 - acc: 0.903 - ETA: 0s - loss: 0.6236 - acc: 0.905 - ETA: 0s - loss: 0.6006 - acc: 0.909 - ETA: 0s - loss: 0.5801 - acc: 0.912 - 3s 2ms/step - loss: 0.5718 - acc: 0.9135 - val_loss: 0.2403 - val_acc: 0.9650\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.24033, saving model to vgg16.hdf5\n",
      "Epoch 2/2\n",
      "2000/2000 [==============================] - ETA: 2s - loss: 0.3375 - acc: 0.850 - ETA: 1s - loss: 0.2016 - acc: 0.950 - ETA: 1s - loss: 0.1148 - acc: 0.972 - ETA: 1s - loss: 0.1538 - acc: 0.973 - ETA: 1s - loss: 0.2250 - acc: 0.967 - ETA: 1s - loss: 0.2371 - acc: 0.969 - ETA: 1s - loss: 0.2842 - acc: 0.966 - ETA: 0s - loss: 0.2805 - acc: 0.967 - ETA: 0s - loss: 0.2757 - acc: 0.965 - ETA: 0s - loss: 0.2627 - acc: 0.964 - ETA: 0s - loss: 0.2850 - acc: 0.958 - ETA: 0s - loss: 0.2619 - acc: 0.960 - ETA: 0s - loss: 0.2993 - acc: 0.959 - ETA: 0s - loss: 0.2821 - acc: 0.959 - ETA: 0s - loss: 0.2775 - acc: 0.960 - ETA: 0s - loss: 0.2729 - acc: 0.962 - ETA: 0s - loss: 0.2561 - acc: 0.964 - ETA: 0s - loss: 0.2592 - acc: 0.964 - ETA: 0s - loss: 0.2624 - acc: 0.963 - ETA: 0s - loss: 0.2499 - acc: 0.964 - ETA: 0s - loss: 0.2381 - acc: 0.966 - ETA: 0s - loss: 0.2282 - acc: 0.966 - ETA: 0s - loss: 0.2293 - acc: 0.966 - ETA: 0s - loss: 0.2340 - acc: 0.966 - ETA: 0s - loss: 0.2308 - acc: 0.966 - 1s 694us/step - loss: 0.2440 - acc: 0.9660 - val_loss: 0.1943 - val_acc: 0.9737\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.24033 to 0.19430, saving model to vgg16.hdf5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1de558b54e0>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpointer = ModelCheckpoint(filepath='vgg16.hdf5', verbose=1, save_best_only=True)\n",
    "\n",
    "model.fit(train_features, train_labels, epochs=2, batch_size=batch_size,\n",
    "          validation_data=(valid_features, valid_labels), verbose=1, callbacks=[checkpointer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model.predict(test_features, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = prediction.clip(0.01, 0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"dogs-vs-cats/sample_submission.csv\")\n",
    "\n",
    "gen = ImageDataGenerator()\n",
    "test_generator = gen.flow_from_directory(\"test2\", (224, 224), shuffle=False, \n",
    "                                         batch_size=16, class_mode=None)\n",
    "\n",
    "for i, fname in enumerate(test_generator.filenames):\n",
    "    index = int(fname[fname.rfind('/')+1:fname.rfind('.')])\n",
    "    df.set_value(index-1, 'label', y_pred[i])\n",
    "\n",
    "df.to_csv('pred.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
